{
  "node_id": "@n8n/n8n-nodes-langchain.lmChatDeepSeek",
  "name": "@n8n/n8n-nodes-langchain.lmChatDeepSeek",
  "version": [
    1
  ],
  "semantic_context": "For advanced usage with an AI chain",
  "parameters": {
    "notice": {
      "displayName": "If using JSON response format, you must include word \"json\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.",
      "name": "notice",
      "type": "notice",
      "default": "",
      "displayOptions": {
        "show": {
          "/options.responseFormat": [
            "json_object"
          ]
        }
      }
    },
    "model": {
      "displayName": "Model",
      "name": "model",
      "type": "options",
      "description": "The model which will generate the completion. <a href=\"https://api-docs.deepseek.com/quick_start/pricing\">Learn more</a>.",
      "typeOptions": {
        "loadOptions": {
          "routing": {
            "request": {
              "method": "GET",
              "url": "/models"
            },
            "output": {
              "postReceive": [
                {
                  "type": "rootProperty",
                  "properties": {
                    "property": "data"
                  }
                },
                {
                  "type": "setKeyValue",
                  "properties": {
                    "name": "={{$responseItem.id}}",
                    "value": "={{$responseItem.id}}"
                  }
                },
                {
                  "type": "sort",
                  "properties": {
                    "key": "name"
                  }
                }
              ]
            }
          }
        }
      },
      "routing": {
        "send": {
          "type": "body",
          "property": "model"
        }
      },
      "default": "deepseek-chat",
      "ai_hint": "The model which will generate the completion. <a href=\"https://api-docs.deepseek.com/quick_start/pricing\">Learn more</a>."
    },
    "options": {
      "displayName": "Options",
      "name": "options",
      "placeholder": "Add Option",
      "description": "Additional options to add",
      "type": "collection",
      "default": {},
      "options": [
        {
          "displayName": "Frequency Penalty",
          "name": "frequencyPenalty",
          "default": 0,
          "typeOptions": {
            "maxValue": 2,
            "minValue": -2,
            "numberPrecision": 1
          },
          "description": "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim",
          "type": "number"
        },
        {
          "displayName": "Maximum Number of Tokens",
          "name": "maxTokens",
          "default": -1,
          "description": "The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).",
          "type": "number",
          "typeOptions": {
            "maxValue": 32768
          }
        },
        {
          "displayName": "Response Format",
          "name": "responseFormat",
          "default": "text",
          "type": "options",
          "options": [
            {
              "name": "Text",
              "value": "text",
              "description": "Regular text response"
            },
            {
              "name": "JSON",
              "value": "json_object",
              "description": "Enables JSON mode, which should guarantee the message the model generates is valid JSON"
            }
          ]
        },
        {
          "displayName": "Presence Penalty",
          "name": "presencePenalty",
          "default": 0,
          "typeOptions": {
            "maxValue": 2,
            "minValue": -2,
            "numberPrecision": 1
          },
          "description": "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics",
          "type": "number"
        },
        {
          "displayName": "Sampling Temperature",
          "name": "temperature",
          "default": 0.7,
          "typeOptions": {
            "maxValue": 2,
            "minValue": 0,
            "numberPrecision": 1
          },
          "description": "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
          "type": "number"
        },
        {
          "displayName": "Timeout",
          "name": "timeout",
          "default": 360000,
          "description": "Maximum amount of time a request is allowed to take in milliseconds",
          "type": "number"
        },
        {
          "displayName": "Max Retries",
          "name": "maxRetries",
          "default": 2,
          "description": "Maximum number of retries to attempt",
          "type": "number"
        },
        {
          "displayName": "Top P",
          "name": "topP",
          "default": 1,
          "typeOptions": {
            "maxValue": 1,
            "minValue": 0,
            "numberPrecision": 1
          },
          "description": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.",
          "type": "number"
        }
      ],
      "ai_hint": "此参数为 fixedCollection，包含嵌套字段，不要扁平化。"
    }
  },
  "configuration_logic": {
    "required_sequence": [
      "notice",
      "notice",
      "model",
      "options"
    ],
    "conditional_rules": [
      "IF /options.responseFormat == 'json_object' THEN SHOW parameter 'notice'"
    ]
  }
}