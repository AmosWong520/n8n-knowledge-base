{
  "node_id": "n8n-nodes-base.perplexity",
  "name": "Perplexity",
  "version": 1,
  "semantic_context": "Interact with the Perplexity API to generate AI responses with citations",
  "natural_language_description": "这是 Perplexity 节点。主要用于: Interact with the Perplexity API to generate AI responses with citations",
  "parameters": {
    "resource": {
      "displayName": "Resource",
      "name": "resource",
      "type": "hidden",
      "required": false,
      "default": "chat",
      "description": "",
      "natural_language_description": "参数名: Resource | 默认值: chat",
      "available_options": [
        "Chat (值: chat)",
        "Custom API Call (值: __CUSTOM_API_CALL__)"
      ]
    },
    "operation": {
      "displayName": "Operation",
      "name": "operation",
      "type": "options",
      "required": false,
      "default": "complete",
      "description": "",
      "natural_language_description": "参数名: Operation | 默认值: complete",
      "available_options": [
        "Message a Model (值: complete) - Create one or more completions for a given text",
        "Custom API Call (值: __CUSTOM_API_CALL__)"
      ]
    },
    "model": {
      "displayName": "Model",
      "name": "model",
      "type": "options",
      "required": true,
      "default": "sonar",
      "description": "The model which will generate the completion",
      "natural_language_description": "参数名: Model | 作用: The model which will generate the completion | 默认值: sonar | (必填项)",
      "available_options": [
        "Sonar (值: sonar)",
        "Sonar Deep Research (值: sonar-deep-research)",
        "Sonar Pro (值: sonar-pro)",
        "Sonar Reasoning (值: sonar-reasoning)",
        "Sonar Reasoning Pro (值: sonar-reasoning-pro)"
      ]
    },
    "messages": {
      "displayName": "Messages",
      "name": "messages",
      "type": "fixedCollection",
      "required": true,
      "default": {
        "message": [
          {
            "role": "user",
            "content": ""
          }
        ]
      },
      "description": "Any optional system messages must be sent first, followed by alternating user and assistant messages",
      "natural_language_description": "参数名: Messages | 作用: Any optional system messages must be sent first, followed by alternating user and assistant messages | 默认值: {'message': [{'role': 'user', 'content': ''}]} | (必填项)",
      "available_options": [
        "message (值: None)"
      ]
    },
    "simplify": {
      "displayName": "Simplify Output",
      "name": "simplify",
      "type": "boolean",
      "required": false,
      "default": false,
      "description": "Whether to return only essential fields (ID, citations, message)",
      "natural_language_description": "参数名: Simplify Output | 作用: Whether to return only essential fields (ID, citations, message) | 默认值: False"
    },
    "options": {
      "displayName": "Options",
      "name": "options",
      "type": "collection",
      "required": false,
      "default": {},
      "description": "",
      "natural_language_description": "参数名: Options | 默认值: {}",
      "available_options": [
        "frequencyPenalty (值: None) - Values greater than 1.0 penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim",
        "maxTokens (值: None) - The maximum number of tokens to generate in the completion. The number of tokens requested plus the number of prompt tokens sent in messages must not exceed the context window token limit of model requested.",
        "temperature (值: None) - The amount of randomness in the response, valued between 0 inclusive and 2 exclusive. Higher values are more random, and lower values are more deterministic.",
        "topK (值: None) - The number of tokens to keep for highest Top K filtering, specified as an integer between 0 and 2048 inclusive. If set to 0, Top K filtering is disabled. We recommend either altering Top K or Top P, but not both.",
        "topP (值: None) - The nucleus sampling threshold, valued between 0 and 1 inclusive. For each subsequent token, the model considers the results of the tokens with Top P probability mass. We recommend either altering Top K or Top P, but not both.",
        "presencePenalty (值: None) - A value between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
        "returnImages (值: None) - Whether or not a request to an online model should return images. Requires Perplexity API usage Tier-2.",
        "returnRelatedQuestions (值: None) - Whether or not a request to an online model should return related questions. Requires Perplexity API usage Tier-2.",
        "searchDomainFilter (值: None) - Limit the citations used by the online model to URLs from the specified domains. For blacklisting, add a `-` to the beginning of the domain string (e.g., `-domain1`). Currently limited to 3 domains. Requires Perplexity API usage Tier-3.",
        "searchRecency (值: None) - Returns search results within the specified time interval"
      ]
    },
    "requestOptions": {
      "displayName": "Request Options",
      "name": "requestOptions",
      "type": "collection",
      "required": false,
      "default": {},
      "description": "",
      "natural_language_description": "参数名: Request Options | 默认值: {}",
      "available_options": [
        "batching (值: None)",
        "allowUnauthorizedCerts (值: None) - Whether to accept the response even if SSL certificate validation is not possible",
        "proxy (值: None) - HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128",
        "timeout (值: None) - Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"
      ]
    }
  },
  "ids_manifest": {
    "role": "processor",
    "input_contract": {
      "preferred_source": "{{ $json.payload.primary }}",
      "accepts_binary": false
    },
    "output_contract": {
      "standardizer_logic": "return { payload: { primary: upstream.choices?.[0]?.message?.content || upstream.message || upstream } }"
    }
  }
}