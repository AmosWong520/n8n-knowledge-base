{
  "node_id": "@n8n/n8n-nodes-langchain.lmChatOllama",
  "name": "Ollama Chat Model",
  "version": 1,
  "semantic_context": "Language Model Ollama",
  "natural_language_description": "这是 Ollama Chat Model 节点。主要用于: Language Model Ollama",
  "parameters": {
    "notice": {
      "displayName": "This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>",
      "name": "notice",
      "type": "notice",
      "required": false,
      "default": "",
      "description": "",
      "natural_language_description": "参数名: This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>"
    },
    "model": {
      "displayName": "Model",
      "name": "model",
      "type": "options",
      "required": true,
      "default": "llama3.2",
      "description": "The model which will generate the completion. To download models, visit Ollama Models Library.",
      "natural_language_description": "参数名: Model | 作用: The model which will generate the completion. To download models, visit Ollama Models Library. | 默认值: llama3.2 | (必填项)"
    },
    "options": {
      "displayName": "Options",
      "name": "options",
      "type": "collection",
      "required": false,
      "default": {},
      "description": "Additional options to add",
      "natural_language_description": "参数名: Options | 作用: Additional options to add | 默认值: {}",
      "available_options": [
        "temperature (值: None) - Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.",
        "topK (值: None) - Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.",
        "topP (值: None) - Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.",
        "frequencyPenalty (值: None) - Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition.",
        "keepAlive (值: None) - Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes).",
        "lowVram (值: None) - Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory.",
        "mainGpu (值: None) - Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs.",
        "numBatch (值: None) - Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage.",
        "numCtx (值: None) - The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model.",
        "numGpu (值: None) - Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection.",
        "numPredict (值: None) - The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs.",
        "numThread (值: None) - Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection.",
        "penalizeNewline (值: None) - Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text",
        "presencePenalty (值: None) - Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity.",
        "repeatPenalty (值: None) - Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty.",
        "useMLock (值: None) - Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory.",
        "useMMap (值: None) - Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled.",
        "vocabOnly (值: None) - Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization.",
        "format (值: None) - Specifies the format of the API response"
      ]
    }
  }
}