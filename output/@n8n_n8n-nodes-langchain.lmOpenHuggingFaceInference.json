{
  "node_id": "@n8n/n8n-nodes-langchain.lmOpenHuggingFaceInference",
  "name": "Hugging Face Inference Model",
  "version": 1,
  "semantic_context": "Language Model HuggingFaceInference",
  "natural_language_description": "这是 Hugging Face Inference Model 节点。主要用于: Language Model HuggingFaceInference",
  "parameters": {
    "notice": {
      "displayName": "This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>",
      "name": "notice",
      "type": "notice",
      "required": false,
      "default": "",
      "description": "",
      "natural_language_description": "参数名: This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>"
    },
    "model": {
      "displayName": "Model",
      "name": "model",
      "type": "string",
      "required": false,
      "default": "mistralai/Mistral-Nemo-Base-2407",
      "description": "",
      "natural_language_description": "参数名: Model | 默认值: mistralai/Mistral-Nemo-Base-2407"
    },
    "options": {
      "displayName": "Options",
      "name": "options",
      "type": "collection",
      "required": false,
      "default": {},
      "description": "Additional options to add",
      "natural_language_description": "参数名: Options | 作用: Additional options to add | 默认值: {}",
      "available_options": [
        "endpointUrl (值: None) - Custom endpoint URL",
        "frequencyPenalty (值: None) - Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim",
        "maxTokens (值: None) - The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).",
        "presencePenalty (值: None) - Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics",
        "temperature (值: None) - Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
        "topK (值: None) - Controls the top tokens to consider within the sample operation to create new text",
        "topP (值: None) - Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both."
      ]
    }
  },
  "ids_manifest": {
    "role": "processor",
    "input_contract": {
      "preferred_source": "{{ $json.payload.primary }}",
      "accepts_binary": false
    },
    "output_contract": {
      "standardizer_logic": "return { payload: { primary: upstream.text || upstream.content || upstream } }"
    }
  }
}